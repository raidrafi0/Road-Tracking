\relax 
\citation{Grigore2000Reinforcement}
\citation{Cohen2010Reinforcement}
\citation{Liu2004Reinforcement}
\citation{Supancic2017Tracking}
\citation{Jinlin2009Neurofuzzy}
\citation{Hall2011Reinforcement}
\citation{Sootla2013On}
\citation{Wei2015Reinforcement}
\citation{Perot2017End}
\citation{Yun2017Action}
\citation{Yun2018Action}
\citation{Grigore2000Reinforcement}
\citation{Cohen2010Reinforcement}
\citation{Liu2004Reinforcement}
\citation{Supancic2017Tracking}
\citation{Jinlin2009Neurofuzzy}
\citation{Sootla2013On}
\citation{Hall2011Reinforcement}
\citation{Wei2015Reinforcement}
\citation{Yun2017Action}
\citation{Yun2018Action}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\citation{Ros2016TheSYNTHIA}
\citation{koren2001computer}
\citation{Arulkumaran2017Deep}
\@writefile{toc}{\contentsline {section}{\numberline {II}Modelling}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Road Tracking:}{2}}
\newlabel{Fig:segmentation_regions1}{{\unhbox \voidb@x \hbox {II-A}}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Actions codes:}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The road tracking actions with their suggested codes and descriptions\relax }}{2}}
\newlabel{Table:Signs_codes}{{I}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Policy search:}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The suggested front view road tracking zones, lines and anchor point\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:segmentation_regions1}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The road tracking motivations of segmented images: (a) the straight forward direction, (b) turning left direction, (c) turning right direction, (d) reverse or backward direction, (e-g) stopping action because of a single crossing object, (h-j) stopping action because of two crossing objects and (k) stopping action because of three crossing objects\relax }}{3}}
\newlabel{Fig:segmentation}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The main design of the proposed DRL-RT. It consists of two conventional layers, two ReLU layers, a pooling layer, a fully connected layer, a regression layer and a classification layer\relax }}{3}}
\newlabel{Fig:Deep_Reinf_Net}{{3}{3}}
\citation{arulkumaran2017brief}
\citation{omar2018deep}
\citation{simo2016learning}
\citation{krizhevsky2012imagenet}
\citation{wu2017introduction}
\citation{stutz2014neural}
\newlabel{Eq:MDP}{{1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-D}}Proposed DRL-RT:}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-E}}Theoretical concepts:}{4}}
\newlabel{eq:conv_layer}{{2}{4}}
\newlabel{eq:relu_layer}{{3}{4}}
\newlabel{eq:pooling_layer}{{4}{4}}
\newlabel{eq:fully_connect}{{5}{4}}
\citation{saugirouglu2009intelligent}
\citation{Ros2016TheSYNTHIA}
\citation{kingma2014adam}
\newlabel{eq:fully_connect}{{6}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Results}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Databases:}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Training stage:}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The training performance of the DRL-RT for the: (1) spring environment, (2) fog environment, (3) rain environment and (4) heavy-rain environment\relax }}{5}}
\newlabel{fig:training_curves}{{4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Testing stage:}{5}}
\citation{Karaduman2017Deep}
\citation{bojarski2016end}
\citation{George2016Real}
\citation{Yun2017Action}
\citation{Yun2018Action}
\citation{mnih2015human}
\citation{Karaduman2017Deep}
\citation{bojarski2016end}
\citation{George2016Real}
\citation{Yun2017Action}
\citation{Yun2018Action}
\citation{mnih2015human}
\citation{arulkumaran2017brief}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Examples of the four employed environments\relax }}{6}}
\newlabel{Table:Environments_Examples}{{II}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The performance of the DRL-RT under different employed environments\relax }}{6}}
\newlabel{fig:Main_Results}{{5}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Comparisons:}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces A comparison between our proposed DRL-RT method and other suggested networks\relax }}{6}}
\newlabel{Table:Comparisons}{{III}{6}}
\bibstyle{IEEEtr}
\bibdata{references14}
\bibcite{Grigore2000Reinforcement}{1}
\bibcite{Cohen2010Reinforcement}{2}
\bibcite{Liu2004Reinforcement}{3}
\bibcite{Supancic2017Tracking}{4}
\bibcite{Jinlin2009Neurofuzzy}{5}
\bibcite{Hall2011Reinforcement}{6}
\bibcite{Sootla2013On}{7}
\bibcite{Wei2015Reinforcement}{8}
\bibcite{Perot2017End}{9}
\bibcite{Yun2017Action}{10}
\bibcite{Yun2018Action}{11}
\bibcite{Ros2016TheSYNTHIA}{12}
\bibcite{koren2001computer}{13}
\bibcite{Arulkumaran2017Deep}{14}
\bibcite{arulkumaran2017brief}{15}
\bibcite{omar2018deep}{16}
\bibcite{simo2016learning}{17}
\bibcite{krizhevsky2012imagenet}{18}
\bibcite{wu2017introduction}{19}
\bibcite{stutz2014neural}{20}
\bibcite{saugirouglu2009intelligent}{21}
\bibcite{kingma2014adam}{22}
\bibcite{Karaduman2017Deep}{23}
\bibcite{bojarski2016end}{24}
\bibcite{George2016Real}{25}
\bibcite{mnih2015human}{26}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Conclusion}{7}}
\@writefile{toc}{\contentsline {section}{References}{7}}
